{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2OUUEECREp5",
        "colab_type": "code",
        "outputId": "8501e457-e60c-4101-c7da-938882a43c62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import pprint\n",
        "import re\n",
        "from lxml import etree\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import FastText\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1B47OiEiG2Lo1jUY6hy_zMmHBxfKQuJ8-'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('ted_en-20160408.xml') \n",
        "\n",
        "targetXML=open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
        "\n",
        "# Getting contents of <content> tag from the xml file\n",
        "target_text = etree.parse(targetXML)\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
        "\n",
        "# Removing \"Sound-effect labels\" using regular expression (i.e. (Audio), (Laughter))\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "\n",
        "# Tokenising the sentence to process it by using NLTK library\n",
        "sent_text=sent_tokenize(content_text)\n",
        "\n",
        "# Removing punctuations and changing all characters to lower case\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "     normalized_text.append(tokens)\n",
        "\n",
        "# Tokenising each sentence to process individual word\n",
        "sentences=[]\n",
        "sentences=[word_tokenize(sentence) for sentence in normalized_text]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6PFAaeQRiUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_4_models(sentences, model, architecture, Formula, positiveList, negativeList):\n",
        "\n",
        "    print('Formula:', Formula)\n",
        "    print('Model:', model)\n",
        "    print('Architecture:', architecture)\n",
        "    print('\\nPositive Words:', positiveList)\n",
        "    print('Negative Words:', negativeList)\n",
        "\n",
        "    if(model == 'FastText' and architecture == 'Skip Gram'):\n",
        "      ft_sg_model = FastText(sentences = sentences, size = 100, window = 5, min_count = 5, workers = 4, sg = 1)\n",
        "      result = ft_sg_model.wv.most_similar(positive = positiveList, negative = negativeList, topn = 1)\n",
        "    elif(model == 'FastText' and architecture == 'CBOW'):\n",
        "      ft_cbow_model = FastText(sentences = sentences, size = 100, window = 5, min_count = 5, workers = 4, sg = 0)\n",
        "      result = ft_cbow_model.wv.most_similar(positive = positiveList, negative = negativeList, topn = 1)\n",
        "    elif(model == 'Word2Vec' and architecture == 'Skip Gram'):\n",
        "      wv_sg_model = Word2Vec(sentences = sentences, size = 100, window = 5, min_count = 5, workers = 4, sg = 1)\n",
        "      result = wv_sg_model.wv.most_similar(positive = positiveList, negative = negativeList, topn = 1)\n",
        "    elif(model == 'Word2Vec' and architecture == 'CBOW'):\n",
        "      wv_cbow_model = Word2Vec(sentences = sentences, size = 100, window = 5, min_count = 5, workers = 4, sg = 0)\n",
        "      result = wv_cbow_model.wv.most_similar(positive = positiveList, negative = negativeList , topn = 1)\n",
        "\n",
        "    return result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Uy-a2y4TgGy",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Word Algebra Calculator \n",
        "\n",
        "#@markdown Please select the model, architecture and formula to calculate the word algebra\n",
        "\n",
        "# Get the input\n",
        "Formula = 'University + Student' #@param {type: \"string\"}\n",
        "processed_Formula = Formula.lower()\n",
        "Model = 'Word2Vec' #@param ['Word2Vec', 'FastText']\n",
        "Architecture = \"Skip Gram\" #@param [\"CBOW\", \"Skip Gram\"]\n",
        "\n",
        "# process formula\n",
        "processed_Formula = processed_Formula.split()\n",
        "\n",
        "# separate positive and negative words\n",
        "positiveList = []\n",
        "negativeList = []\n",
        "\n",
        "positiveList.insert(0, processed_Formula[0])\n",
        "\n",
        "for x in range(len(processed_Formula)):\n",
        "  if(processed_Formula[x] == '+'):\n",
        "    positiveList.append(processed_Formula[x+1])\n",
        "  elif(processed_Formula[x] == '-'):\n",
        "    negativeList.append(processed_Formula[x+1])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV8GfgNVXUlD",
        "colab_type": "code",
        "outputId": "f4f7652e-9eaf-4885-ea6e-49d92027f850",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Call the function\n",
        "result = get_4_models(sentences, Model, Architecture, Formula, positiveList, negativeList)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Formula: University + Student\n",
            "Model: Word2Vec\n",
            "Architecture: Skip Gram\n",
            "\n",
            "Positive Words: ['university', 'student']\n",
            "Negative Words: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-dcOE2Dd0s1",
        "colab_type": "code",
        "outputId": "e2338e0f-6816-49c7-a005-d6ef7a3ff3f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# print the final result\n",
        "print('Result:', result[0][0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Result: harvard\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}