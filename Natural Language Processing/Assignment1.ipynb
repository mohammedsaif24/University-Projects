{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLMKqe-n-8g-",
        "colab_type": "code",
        "outputId": "d1ffbc28-47ea-4b2e-a5db-399c0488cefc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# import libraries\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "from pprint import pprint"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PweHL22f_D-N",
        "colab_type": "code",
        "outputId": "ebe06637-2d1d-42c9-926d-50db2aeb31de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!pip install wikipedia\n",
        "import wikipedia"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.21.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hU5WWcN_EqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cities = [\"Canberra\",\"Perth\", \"Sydney\", \"Melbourne\", \"Darwin,_Northern_Territory\", \"Brisbane\", \"Hobart\"]\n",
        "corpus = [wikipedia.page(city).content for city in cities]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTx5HWAybmEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculate the DF for all documents\n",
        "\n",
        "def calculateDF(tokenized_docs):\n",
        "  DF = {} \n",
        "  for tokensized_doc in tokenized_docs:\n",
        "    # get each unique word in the doc - we need to know whether the word is appeared in the document\n",
        "    for term in np.unique(tokensized_doc):\n",
        "      try:\n",
        "        DF[term] +=1\n",
        "      except:\n",
        "        DF[term] =1\n",
        "  \n",
        "  return DF"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCvjBPydccup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculate TF-IDF for all documents\n",
        "\n",
        "def calculateTF_IDF(tokenized_docs, DF):\n",
        "  TF_IDF = {}\n",
        "  tf = {}\n",
        "  \n",
        "  # total number of documents\n",
        "  N = len(tokenized_docs)\n",
        "\n",
        "  doc_id = 0\n",
        "  # get each tokenised doc\n",
        "  for tokensized_doc in tokenized_docs:\n",
        "      # initialise counter for the doc\n",
        "      counter = Counter(tokensized_doc)\n",
        "      # calculate total number of words in the doc\n",
        "      total_num_words = len(tokensized_doc)    \n",
        "\n",
        "      # get each unique word in the doc\n",
        "      for term in np.unique(tokensized_doc):\n",
        "\n",
        "          #calculate Term Frequency \n",
        "          tf[doc_id, term] = counter[term]/total_num_words\n",
        "          \n",
        "          #calculate Document Frequency\n",
        "          df = DF[term]\n",
        "\n",
        "          # calculate Inverse Document Frequency\n",
        "          idf = math.log(N/(df+1))+1\n",
        "\n",
        "          # calculate TF-IDF\n",
        "          TF_IDF[doc_id, term] = tf[doc_id, term] * idf\n",
        "\n",
        "      doc_id += 1\n",
        "  return TF_IDF, tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzNDpXQa4pbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sort tf and tf-idf values\n",
        "def sort(TF, TF_IDF, top_n):\n",
        "  sorted_TF = sorted(TF.items(), key=lambda x: x[1], reverse=True)[: top_n]\n",
        "  sorted_TFIDF = sorted(TF_IDF.items(), key=lambda x: x[1], reverse=True)[: top_n]\n",
        "  return sorted_TF, sorted_TFIDF"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HX1PxPy_LRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#call the funtion\n",
        "\n",
        "def get_tf_and_idf(corpus, top_n):\n",
        "  #Process the corpus (incl. tokenisation, lower_case, stopword removal)\n",
        "  sorted_TF = {}\n",
        "  sorted_TFIDF = {}\n",
        "  sww = sw.words()\n",
        "  tokenized_docs=[]\n",
        "  \n",
        "  for doc in corpus:\n",
        "    #preprocessing on all the 7 docs\n",
        "    clean_doc = re.sub(r'[^\\w\\s]','', doc)\n",
        "    tokenized_sentence = sent_tokenize(clean_doc.lower())\n",
        "    lower_case = word_tokenize(clean_doc.lower())\n",
        "    stopword_removal = [w for w in lower_case if not w in sww]\n",
        "    tokenized_docs.append(stopword_removal)\n",
        "\n",
        "  #rest of your code here\n",
        "  # get DF values\n",
        "  DF = calculateDF(tokenized_docs)\n",
        "\n",
        "  # get TF-IDF & TF values\n",
        "  TF_IDF, TF = calculateTF_IDF(tokenized_docs, DF)\n",
        "  \n",
        "  # sort and get top N TF-IDF and TF values\n",
        "  sorted_TF, sorted_TFIDF = sort(TF, TF_IDF, top_n)\n",
        "\n",
        "  # printing all the list\n",
        "  print('Total docs in corpus:', len(tokenized_docs))\n",
        "  print('\\nTop 10 of tf values:')\n",
        "  print('(doc id, word): tf')\n",
        "  pprint(sorted_TF)\n",
        "  #print('=======================================')\n",
        "  print('\\nTop 10 of tf values:')\n",
        "  print('(doc id, word): tf*idf')\n",
        "  pprint(sorted_TFIDF)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjYverHSsMjj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "dd0f2b62-5361-4c2e-a5ff-8b2dd907fa35"
      },
      "source": [
        "get_tf_and_idf(corpus, 10)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total docs in corpus: 7\n",
            "\n",
            "Top 10 of tf values:\n",
            "(doc id, word): tf\n",
            "[((5, 'brisbane'), 0.046163190929337926),\n",
            " ((4, 'darwin'), 0.04120879120879121),\n",
            " ((3, 'melbourne'), 0.03969006957621758),\n",
            " ((1, 'perth'), 0.03878231859883236),\n",
            " ((2, 'sydney'), 0.03666121112929623),\n",
            " ((6, 'hobart'), 0.03344575604272063),\n",
            " ((0, 'canberra'), 0.02618181818181818),\n",
            " ((3, 'city'), 0.014705882352941176),\n",
            " ((5, 'city'), 0.014375379631504353),\n",
            " ((6, 'city'), 0.013771781899943788)]\n",
            "\n",
            "Top 10 of tf values:\n",
            "(doc id, word): tf*idf\n",
            "[((4, 'darwin'), 0.055074405355269765),\n",
            " ((5, 'brisbane'), 0.05327927819409089),\n",
            " ((1, 'perth'), 0.044760639376119696),\n",
            " ((6, 'hobart'), 0.04469932438390229),\n",
            " ((2, 'sydney'), 0.03666121112929623),\n",
            " ((0, 'canberra'), 0.03499127310426448),\n",
            " ((3, 'melbourne'), 0.03439019931234105),\n",
            " ((5, 'queensland'), 0.015189267906210122),\n",
            " ((4, 'darwins'), 0.013409303383901),\n",
            " ((5, 'brisbanes'), 0.013227399491064117)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}