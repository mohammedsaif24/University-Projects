{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "APAT0532_COMP5046_Ass1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGHoy6KpQDfZ",
        "colab_type": "text"
      },
      "source": [
        "# COMP5046 Assignment 1\n",
        "*Make sure you change the file name with your unikey.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTf21j_oQIiD",
        "colab_type": "text"
      },
      "source": [
        "# Readme\n",
        "\n",
        "**Run the cells in which the following comment is written.**\n",
        "\n",
        "`#**********************************RUN THIS CELL**********************************#`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXbQohXLKSgO",
        "colab_type": "text"
      },
      "source": [
        "***Visualising the comparison of different results is a good way to justify your decision.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34DVNKgqQY21",
        "colab_type": "text"
      },
      "source": [
        "# 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cWUxAQrGlq6",
        "colab_type": "text"
      },
      "source": [
        "## 1.1. Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7C4snIcNl22",
        "colab_type": "code",
        "outputId": "da50b76f-6483-4b74-9dad-797236d9a80f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1vF3FqgBC1Y-RPefeVmY8zetdZG1jmHzT'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('imdb_train.csv')\n",
        "\n",
        "id = '1XhaV8YMuQeSwozQww8PeyiWMJfia13G6'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('imdb_test.csv')\n",
        "\n",
        "import pandas as pd\n",
        "df_train = pd.read_csv(\"imdb_train.csv\")\n",
        "df_test = pd.read_csv(\"imdb_test.csv\")\n",
        "\n",
        "reviews_train = df_train['review'].tolist()\n",
        "sentiments_train = df_train['sentiment'].tolist()\n",
        "reviews_test = df_test['review'].tolist()\n",
        "sentiments_test = df_test['sentiment'].tolist()\n",
        "\n",
        "print(\"Training set number:\",len(reviews_train))\n",
        "print(\"Testing set number:\",len(reviews_test))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set number: 25000\n",
            "Testing set number: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seQX9CHiFsWw",
        "colab_type": "text"
      },
      "source": [
        "## 1.2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUrZPX-iF0Rm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Import libraries for pre processing\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords as sw\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from itertools import chain\n",
        "\n",
        "# Import libraries for model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# You can enable GPU here (cuda); or just CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9gBSgBCQh24",
        "colab_type": "text"
      },
      "source": [
        "## 1.3. Preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RdKI8E2KRwe",
        "colab_type": "text"
      },
      "source": [
        "*Justification:*\n",
        "\n",
        "**PREPROCESSING:**\n",
        "\n",
        "**CLEAN DATA**\n",
        "\n",
        "*To clean data, all the HTML tags from the text must be removed before processing it. IMDB data contains only `<br/ >` tags. To remove these `<br/ >` tags, I replaced all the `<br/ >` tags in the training and testing dataset with space.*\n",
        "\n",
        "*Regular Expression describes the search pattern in a text. I have used regular expression to remove all the puntuations, numbers and special characters from the text. Anything which is not a character or a space is removed from the text.*\n",
        "\n",
        "**CASE FOLDING**\n",
        "\n",
        "*Case Folding is done so that the text can be compared irrespective of it's case. I have converted all my text into lower case for better comparison and processing of the model.*\n",
        "\n",
        "**TOKENIZATION**\n",
        "\n",
        "*Tokenization is a process of breaking sentences into tokens such as words, phrases, etc. Each of these tokens have some value associated to it and are used for processing. I have used these tokens as the input to the model.*\n",
        "\n",
        "**STOP WORDS REMOVAL**\n",
        "\n",
        "*Stop words are the words which do not contribute to the meaning or the context of the sentence, for example \"a\", \"an\", \"the\", etc. Hence they should be removed as they only increase the size of the database and does not affect precision or recall in any way. Hence, stop words removal is used as a preprocessing technique. I have used the list of stop words from \"English nltk\" to remove them from data set.*\n",
        "\n",
        "**LEMMATIZATION**\n",
        "\n",
        "*Lemmatisation is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. It converts all the words to its roots and returns the base word, example: \"geese\" to \"goose\". It provides precision to the dataset and improves recall.*\n",
        "\n",
        "**LABEL ENCODING**\n",
        "\n",
        "*Label Encoding is used to convert the text data into model understandable language. It converts each value to a number. The categorical data must be encoded to numbers before using it in a model. Hence, I have used label encoding to encode the labels from training and testing data set, \"neg\" label to \"0\" and \"pos\" label to 1.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emyl1lWxGr12",
        "colab_type": "code",
        "outputId": "911b7827-1bf4-4d0c-d145-bdf2fc30d35c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# PREPROCESSING\n",
        "def cleanData(x):\n",
        "\n",
        "  # Remove all the <br/> tags and replace them with space. \n",
        "  # (Only <br/> tags removed as there are no other tags present in the database)\n",
        "  x = x.replace(\"<br />\", \" \")\n",
        "\n",
        "  # Remove everything that is not a-z, A-Z, or a space\n",
        "  # Remove numbers, puntuations, accented characters, underscores as well\n",
        "  x = re.sub(r'[^a-zA-Z\\s]', ' ', x)\n",
        "  return x\n",
        "\n",
        "# CLEAN DATA\n",
        "\n",
        "# Remove puntuations and HTML tags for training data set\n",
        "reviews_train = [cleanData(s) for s in reviews_train]\n",
        "\n",
        "# Remove puntuations and HTML tags for testing data set\n",
        "reviews_test = [cleanData(s) for s in reviews_test]\n",
        "\n",
        "# CASE FOLDING\n",
        "\n",
        "# Convert training data set to lower case \n",
        "reviews_train = [s.lower() for s in reviews_train]\n",
        "\n",
        "# Convert testing data set to lower case \n",
        "reviews_test = [s.lower() for s in reviews_test]\n",
        "#--------------------------------------------------------------------#\n",
        "\n",
        "# TOKENIZATION\n",
        "# Tokenization is used for splitting your data set into tokens.\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenize the training data set\n",
        "reviews_train = [word_tokenize(s) for s in reviews_train]\n",
        "\n",
        "# Tokenize the testing data set\n",
        "reviews_test = [word_tokenize(s) for s in reviews_test]\n",
        "#--------------------------------------------------------------------#\n",
        "\n",
        "#STOP WORDS REMOVAL\n",
        "# Stop words are the most common words used in any natural language. \n",
        "# These words don't add value to the meaning of the senetence.\n",
        "# Hence, they must be removed from your data set.\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = sw.words(\"english\")\n",
        "\n",
        "# Remove stop words from training data set\n",
        "reviews_train_ns=[]\n",
        "for tokens in reviews_train:\n",
        "    filtered_sentence1 = [w1 for w1 in tokens if not w1 in stop_words]\n",
        "    reviews_train_ns.append(filtered_sentence1)\n",
        "\n",
        "# Remove stop words from testing data set\n",
        "reviews_test_ns=[]\n",
        "for tokens in reviews_test:\n",
        "    filtered_sentence2 = [w2 for w2 in tokens if not w2 in stop_words]\n",
        "    reviews_test_ns.append(filtered_sentence2)\n",
        "#--------------------------------------------------------------------#\n",
        "\n",
        "# LEMMETIZATION\n",
        "# Lemmatisation is the process of grouping together the inflected forms of a word so they \n",
        "# can be analysed as a single item, identified by the word's lemma, or dictionary form.\n",
        "\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmetize training data set\n",
        "reviews_train_le = []\n",
        "for tokens in reviews_train_ns:\n",
        "    lemma_sentence1 = [lemmatizer.lemmatize(w1) for w1 in tokens ]\n",
        "    reviews_train_le.append(lemma_sentence1)\n",
        "\n",
        "# Lemmetize testing data set\n",
        "reviews_test_le = []\n",
        "for tokens in reviews_test_ns:\n",
        "    lemma_sentence2 = [lemmatizer.lemmatize(w2) for w2 in tokens ]\n",
        "    reviews_test_le.append(lemma_sentence2)\n",
        "#--------------------------------------------------------------------#\n",
        "\n",
        "#LABEL ENCODING\n",
        "\n",
        "# Encoding the given labels by label encoder.\n",
        "#  neg = 0, pos = 1\n",
        "\n",
        "labels = np.unique(sentiments_train)\n",
        "\n",
        "lEnc = LabelEncoder()\n",
        "lEnc.fit(labels)\n",
        "\n",
        "# Encoding training labels\n",
        "sentiments_train_encoded = lEnc.transform(sentiments_train)\n",
        "\n",
        "# Encoding testing labels\n",
        "sentiments_test_encoded = lEnc.transform(sentiments_test)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIu_lkJwQ55g",
        "colab_type": "text"
      },
      "source": [
        "# 2 - Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daDvAftceIvr",
        "colab_type": "text"
      },
      "source": [
        "## 2.1. Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "his3ARPJ1etT",
        "colab_type": "text"
      },
      "source": [
        "*Justification:*\n",
        "\n",
        "**WORD2VEC**\n",
        "\n",
        "*Word2Vec is used for better representation of words. It detects the similarities between the words and group similar words together in vector space. Every word is represented by a vector*\n",
        "\n",
        "**CONTINUOUS BAG OF WORDS (CBOW)**\n",
        "\n",
        "*CBOW predicts the center word from context words. CBOW does not work well with the infrequent words as they do not appear frequently in the context words.*\n",
        "\n",
        "**SKIPGRAM MODEL**\n",
        "\n",
        "*CBOW predicts context words given the center word. Skip gram is better for infrequent words. It even works well on small amount of data. Hence, I have used skip gram instead of CBOW so that the model will get trained for rare words as well.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXgFpxIgl-_G",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.1. Data Preprocessing for Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XY4hf9_1tUb",
        "colab_type": "text"
      },
      "source": [
        "*Justification:*\n",
        "\n",
        "**PREPROCESSING FOR WORD EMBEDDING MODEL**\n",
        "\n",
        "*In training input, we have a list of list (multiple reviews in one reviews). To do the word embeddings for every word, I have flattened the list into a single list by using \"chain\" function. From this single list, I have extracted the uniques words by forming a set. I appended `<PAD>` in list of unique words as later on in the Sequence to Sequence Model the initial list of lists will be padded by word `<PAD>`. I also appended `<OOV>` in the list as this will be required to handle the out of vocabulary words.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LByzHLiNinu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "training_data = []\n",
        "testing_data = []\n",
        "\n",
        "# Converting multiple lists into a single list for training data set \n",
        "training_data = list(chain(*[i for i in reviews_train_le]))\n",
        "\n",
        "# Getting unique words from a list and then sorting the list\n",
        "training_set_list = list(set(training_data))\n",
        "training_set_list.sort()\n",
        "\n",
        "# Appending the <PAD> to get it's word embedding\n",
        "training_set_list.append('<PAD>')\n",
        "\n",
        "# Appending the <OOV> to get word embeddings for out of vocabulary words\n",
        "training_set_list.append('<OOV>')\n",
        "\n",
        "# Make dictionary so that we can be reference each index of unique word\n",
        "training_word_dict = {w: i for i, w in enumerate(training_set_list)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhAgWf_AmbZ8",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.2. Build Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVPuwWgvNjOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create skipgrams\n",
        "skip_grams = []\n",
        "\n",
        "for i in range(1, len(training_data) - 1):\n",
        "  # Context size = 2\n",
        "  target = training_word_dict[training_data[i]]\n",
        "  context = [training_word_dict[training_data[i - 1]], training_word_dict[training_data[i + 1]]]\n",
        "\n",
        "  # skipgrams - (target, context[0]), (target, context[1])..\n",
        "  for w in context:\n",
        "      skip_grams.append([target, w])\n",
        "\n",
        "# Prepare random batches from skip-gram\n",
        "def prepare_batch(data, size):\n",
        "  random_inputs = []\n",
        "  random_labels = []\n",
        "  random_index = np.random.choice(range(len(data)), size, replace=False)\n",
        "\n",
        "  for i in random_index:\n",
        "      input_temp = [0]*vocab_size\n",
        "      input_temp[data[i][0]] = 1\n",
        "      random_inputs.append(input_temp)  # target\n",
        "      random_labels.append(data[i][1])  # context word\n",
        "\n",
        "  return np.array(random_inputs), np.array(random_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKalw5lkBMa3",
        "colab_type": "text"
      },
      "source": [
        "*Justification:*\n",
        "\n",
        "**HYPERPARAMETERS:**\n",
        "\n",
        "**Learning Rate: 0.03**\n",
        "\n",
        "*- As with 0.01 learning rate, the model was learning very slow and loss kept on increasing. With learning rate greater than 0.1, model was training very fast and was overfitting. That is why I chose the optimal learning rate of 0.03 so that the model does not get stuck or converge too quickly.*\n",
        "\n",
        "**Batch Size: 1000**\n",
        "\n",
        "*- Batch Size is 1000 as the data is too huge and most of the data should be used for our model to train properly.*\n",
        "\n",
        "**Embedding Size: 34**\n",
        "\n",
        "*- 34 as in the character embedding model, the character array is of 34 characters.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzKSTfL3gPXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "#HYPERPARAMETERS\n",
        "vocab_size = len(training_set_list) # no of unique words in training data set\n",
        "learning_rate_wordEmb = 0.03 # learning rate\n",
        "batchSize_wordEmb = 1000 # batch size\n",
        "embedding_size = 34 # embedding size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_XBCztO6r9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Skip Gram Model\n",
        "class SkipGramModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.linear1 = nn.Linear(vocab_size, embedding_size, bias = False)\n",
        "        self.linear2 = nn.Linear(embedding_size, vocab_size, bias = False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = self.linear1(x) #z1\n",
        "        out = self.linear2(F.relu(hidden)) #zout\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNys5HOdISK-",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.3. Train Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV14xH4dIHCi",
        "colab_type": "code",
        "outputId": "76d7dc49-f139-42b4-f91e-89862cfb1b85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "skip_gram_model = SkipGramModel()\n",
        "\n",
        "# Cross Entropy is used so that the decision boundary is large.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Adam optimizer can handle sparse gradients and provide high F1 score as well\n",
        "optimiser = optim.Adam(skip_gram_model.parameters(), lr = learning_rate_wordEmb)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(1500):\n",
        "\n",
        "    # Make batches\n",
        "    inputs,labels = prepare_batch(skip_grams, batchSize_wordEmb)\n",
        "\n",
        "    # Convert input and labels into torch\n",
        "    inputs_torch = torch.from_numpy(inputs).float()\n",
        "    labels_torch = torch.from_numpy(labels)\n",
        "    \n",
        "    # Train the model\n",
        "    skip_gram_model.train()\n",
        "\n",
        "    # Zero grad\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    # Forward propagation\n",
        "    outputs = skip_gram_model(inputs_torch)\n",
        "  \n",
        "    # Calculate loss\n",
        "    loss = criterion(outputs, labels_torch)\n",
        "\n",
        "    # Back propagation\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "    if epoch % 100 == 99:\n",
        "        print('Epoch: %d, loss: %.4f' %(epoch + 1, loss))    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100, loss: 8.9823\n",
            "Epoch: 200, loss: 8.7778\n",
            "Epoch: 300, loss: 8.6844\n",
            "Epoch: 400, loss: 8.8544\n",
            "Epoch: 500, loss: 9.0072\n",
            "Epoch: 600, loss: 8.8038\n",
            "Epoch: 700, loss: 8.9635\n",
            "Epoch: 800, loss: 9.0254\n",
            "Epoch: 900, loss: 9.0590\n",
            "Epoch: 1000, loss: 8.8086\n",
            "Epoch: 1100, loss: 9.0261\n",
            "Epoch: 1200, loss: 8.8473\n",
            "Epoch: 1300, loss: 8.9149\n",
            "Epoch: 1400, loss: 8.7768\n",
            "Epoch: 1500, loss: 8.7186\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMCv3YI1IfUo",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.4. Save Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "492UMP1xTtM_",
        "colab_type": "code",
        "outputId": "cac689b8-5583-421f-e209-48adb37a7d7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFca2bRXUMev",
        "colab_type": "code",
        "outputId": "89d9c3c6-24d3-4520-d0ba-7a7f629e342f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Save the Skip Gram Model\n",
        "torch.save(skip_gram_model, '/content/gdrive/My Drive/Models/word_embedding_model.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type SkipGramModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b26CKmgGUUB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1jGiRUbEJmaChHgUf15WwIg3DPh80z_ll'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('word_embedding_model.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn16xrDrIs8B",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.5. Load Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IebpYFsIvgh",
        "colab_type": "code",
        "outputId": "c24ad64e-ff55-4ad5-f828-36be8536c78e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Load the model\n",
        "wordEmbeddingModel = torch.load('word_embedding_model.pt')\n",
        "\n",
        "# Evaluate the model\n",
        "wordEmbeddingModel.eval()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SkipGramModel(\n",
              "  (linear1): Linear(in_features=65520, out_features=34, bias=False)\n",
              "  (linear2): Linear(in_features=34, out_features=65520, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLbrXfXvPAss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Extract the weights from the models\n",
        "\n",
        "weight2 = wordEmbeddingModel.linear2.weight\n",
        "word_embeddings = weight2.detach().numpy()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T0ap96aeGlIk"
      },
      "source": [
        "## 2.2. Character Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d16v3oKaGlI0"
      },
      "source": [
        "### 2.2.1. Data Preprocessing for Character Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGmEIYZQGiTj",
        "colab_type": "text"
      },
      "source": [
        "*Justification:*\n",
        "\n",
        "**PREPROCESSING FOR CHARACTER EMBEDDING MODEL**\n",
        "\n",
        "*For preprocessing, I have padded every word in the unique training data word list by \"#\". This was done so that every word will be of equal length which will help model to learn better.*\n",
        "\n",
        "*Also, added 'P', 'A', 'D', 'O', 'V', '<', '>' in character array as the word list also contains `<PAD>` to handle padding and `<OOV>` to handle out of vocabulary words. *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i2CUCL1cGlI2",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Function to add padding to all the words to make them of equal length\n",
        "# Add # at the last of each words to make them of equal length\n",
        "\n",
        "def add_padding_to_words(corpus, seq_length):\n",
        "    output = []\n",
        "    for word in corpus:\n",
        "        if len(word) > seq_length:\n",
        "            output.append(word[:seq_length])\n",
        "        else:\n",
        "            for j in range(seq_length-len(word)):\n",
        "                word = word + \"#\"\n",
        "            output.append(word)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwfOkRuF7mWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Get the length of words having maximum characters\n",
        "maxword_length_train = len(max(training_set_list, key=len))\n",
        "\n",
        "# Add paddings to all your words\n",
        "train_char_pad = add_padding_to_words(training_set_list, maxword_length_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zgiOPcsTGlI6"
      },
      "source": [
        "### 2.2.2. Build Character Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jj3YZ3PWGlI8",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Assume that we have the following character instances\n",
        "# Characters in <PAD> and <OOV> are also added in this as we have it in out dictionary\n",
        "char_arr = ['a', 'b', 'c', 'd', 'e', 'f', 'g',\n",
        "            'h', 'i', 'j', 'k', 'l', 'm', 'n',\n",
        "            'o', 'p', 'q', 'r', 's', 't', 'u',\n",
        "            'v', 'w', 'x', 'y', 'z', '#', '<', \n",
        "            'P', 'A', 'D', '>', 'O', 'V']\n",
        "\n",
        "# Create a dictionary for above char_arr\n",
        "char_dict = {n: i for i, n in enumerate(char_arr)}\n",
        "# Get the dictionary length\n",
        "charDict_len = len(char_dict)\n",
        "\n",
        "# Get one-hot encoding for every word\n",
        "def encode_words(seq_data):\n",
        "  input_batch = []\n",
        "    \n",
        "  for seq in seq_data:\n",
        "    input_data = [char_dict[n] for n in seq]\n",
        "    input_batch.append(np.eye(charDict_len)[input_data])\n",
        "  return input_batch\n",
        "\n",
        "def generate_batch(input_embeddings, label, batch_size):\n",
        "  idx = np.random.choice(input_embeddings.shape[0], size=batch_size, replace = False)\n",
        "  return input_embeddings[idx,:,:],label[idx, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzAbNfH8CEoe",
        "colab_type": "text"
      },
      "source": [
        "*Justification:*\n",
        "\n",
        "**HYPERPARAMETERS:**\n",
        "\n",
        "**Learning Rate: 0.05**\n",
        "\n",
        "*- As with 0.01 learning rate, the model was learning very slow and loss kept on increasing. With learning rate greater than 0.1, model was training very fast and was overfitting. That is why I chose the optimal learning rate of 0.05 so that the model does not get stuck or converge too quickly.*\n",
        "\n",
        "**Batch Size: 1000**\n",
        "\n",
        "*- Batch Size is 1000 as the data is too huge and most of the data should be used for our model to train properly.*\n",
        "\n",
        "**Number of hidden layers: 20**\n",
        "\n",
        "*- 20 so that the embedding size if not too big for each character.*\n",
        "\n",
        "**n_input, n_class: 34**\n",
        "\n",
        "*- length of character array*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzjRuN9KNrhK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# HYPERPARAMETERS\n",
        "\n",
        "learning_rate_charEmb = 0.05 # Learning Rate\n",
        "n_hidden = 20 # Number of Hidden Layers\n",
        "n_input = charDict_len # Number of inputs = 34\n",
        "n_class = charDict_len # Number of classes = 34\n",
        "batchSize_charEmb = 1000 # Batch Size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbtpdELM9-HA",
        "colab_type": "code",
        "outputId": "9c2e0708-a37f-430a-941d-bdbf2a444936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Bi-LSTM Model for character based word embeddings\n",
        "\n",
        "class CharacterEmbedding_Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CharacterEmbedding_Model, self).__init__()\n",
        "    self.lstm = nn.LSTM(n_input, n_hidden, batch_first =True,bidirectional=True, dropout=0.2)\n",
        "    self.linear = nn.Linear(n_hidden*2, n_class)\n",
        "\n",
        "  def forward(self, sentence):\n",
        "    \n",
        "    # h_n of shape (num_layers * num_directions, batch, hidden_size)\n",
        "    # tensor containing the hidden state for t = seq_len.\n",
        "    lstm_out, (h_n,c_n) = self.lstm(sentence)\n",
        "\n",
        "    # Concatenate the last hidden state from two directions\n",
        "    hidden_out =torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n",
        "    z = self.linear(hidden_out)\n",
        "    log_output = F.log_softmax(z, dim=1)\n",
        "    return log_output,hidden_out\n",
        "\n",
        "character_embedding_model = CharacterEmbedding_Model()\n",
        "\n",
        "# Loss function and optimizer\n",
        "# MSE loss is sum of squared distance between output and predicted results \n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Adam optimizer can handle sparse gradients and provide high F1 score as well\n",
        "optimizer = optim.Adam(character_embedding_model.parameters(), lr=learning_rate_charEmb)\n",
        "\n",
        "# Prepare input by encoding the words\n",
        "input_batch = encode_words(train_char_pad)\n",
        "\n",
        "# Target will be the word embeddings from the skip gram model\n",
        "target_batch = word_embeddings\n",
        "\n",
        "# Convert input into tensors and target to tensors\n",
        "input_batch_torch = torch.from_numpy(np.array(input_batch)).float()\n",
        "target_batch_torch = torch.from_numpy(np.array(target_batch)).float()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "46W0zFfWGlI_"
      },
      "source": [
        "### 2.1.4. Train Character Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyJbaPfVs2-U",
        "colab_type": "code",
        "outputId": "4a82b9e0-7462-4c7e-e89a-8beb629a966d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "# Train the model\n",
        "\n",
        "for epoch in range(5000):  \n",
        "    \n",
        "  character_embedding_model.train()\n",
        "\n",
        "  # Make batches\n",
        "  batch_input, batch_output = generate_batch(input_batch_torch, target_batch_torch, batchSize_charEmb)\n",
        "\n",
        "  # Forward + Backward + Optimize\n",
        "  outputs,_ = character_embedding_model(batch_input)\n",
        "\n",
        "  # Calculate loss\n",
        "  loss = criterion(outputs, batch_output)\n",
        "\n",
        "  # Back propagation\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # Zero Grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoch % 100 == 99:\n",
        "      print('Epoch: %d, loss: %.4f' %(epoch + 1, loss.item()))    \n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100, loss: 7.7940\n",
            "Epoch: 200, loss: 8.0034\n",
            "Epoch: 300, loss: 7.8183\n",
            "Epoch: 400, loss: 8.1443\n",
            "Epoch: 500, loss: 8.0240\n",
            "Epoch: 600, loss: 8.0723\n",
            "Epoch: 700, loss: 7.9003\n",
            "Epoch: 800, loss: 7.7201\n",
            "Epoch: 900, loss: 7.7944\n",
            "Epoch: 1000, loss: 8.5158\n",
            "Epoch: 1100, loss: 7.8755\n",
            "Epoch: 1200, loss: 8.0678\n",
            "Epoch: 1300, loss: 7.6818\n",
            "Epoch: 1400, loss: 7.5123\n",
            "Epoch: 1500, loss: 7.9060\n",
            "Epoch: 1600, loss: 8.2075\n",
            "Epoch: 1700, loss: 7.7961\n",
            "Epoch: 1800, loss: 8.0386\n",
            "Epoch: 1900, loss: 7.6458\n",
            "Epoch: 2000, loss: 8.0211\n",
            "Epoch: 2100, loss: 7.9764\n",
            "Epoch: 2200, loss: 7.8107\n",
            "Epoch: 2300, loss: 7.5797\n",
            "Epoch: 2400, loss: 7.8151\n",
            "Epoch: 2500, loss: 7.9793\n",
            "Epoch: 2600, loss: 7.8212\n",
            "Epoch: 2700, loss: 7.6934\n",
            "Epoch: 2800, loss: 7.6424\n",
            "Epoch: 2900, loss: 7.3485\n",
            "Epoch: 3000, loss: 7.7182\n",
            "Epoch: 3100, loss: 7.8717\n",
            "Epoch: 3200, loss: 7.7502\n",
            "Epoch: 3300, loss: 7.7979\n",
            "Epoch: 3400, loss: 7.7241\n",
            "Epoch: 3500, loss: 7.6360\n",
            "Epoch: 3600, loss: 7.7643\n",
            "Epoch: 3700, loss: 7.9334\n",
            "Epoch: 3800, loss: 7.9570\n",
            "Epoch: 3900, loss: 7.9529\n",
            "Epoch: 4000, loss: 7.7611\n",
            "Epoch: 4100, loss: 7.7732\n",
            "Epoch: 4200, loss: 7.6244\n",
            "Epoch: 4300, loss: 7.5111\n",
            "Epoch: 4400, loss: 7.8173\n",
            "Epoch: 4500, loss: 7.6961\n",
            "Epoch: 4600, loss: 7.6776\n",
            "Epoch: 4700, loss: 7.6624\n",
            "Epoch: 4800, loss: 7.6470\n",
            "Epoch: 4900, loss: 7.3750\n",
            "Epoch: 5000, loss: 7.4704\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R5Bym9bBGlJE"
      },
      "source": [
        "### 2.1.5. Save Character Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ggTsYIm7GlJF",
        "outputId": "02b9243e-626d-47ee-ca71-426db60f7519",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O60nQvpAqqT1",
        "colab_type": "code",
        "outputId": "b8ecaf88-3c9e-4952-b049-834f4b15e492",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Save the Bi-LSTM Model\n",
        "torch.save(character_embedding_model, '/content/gdrive/My Drive/Models/character_embedding_model.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type CharacterEmbedding_Model. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiSXhAfFqoFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1-0TXvRdSIF1vOgwMFj4Q4__zCdmF5VyN'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('character_embedding_model.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JwOI-wIKGlJI"
      },
      "source": [
        "### 2.1.6. Load Character Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-jyj-lOHWWj",
        "colab_type": "code",
        "outputId": "6906b6ea-4911-46f9-cb7c-71099fb5bd19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Load the model\n",
        "characterEmbeddingModel = torch.load('character_embedding_model.pt')\n",
        "\n",
        "# Evaluate the model\n",
        "characterEmbeddingModel.eval()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharacterEmbedding_Model(\n",
              "  (lstm): LSTM(34, 20, batch_first=True, dropout=0.2, bidirectional=True)\n",
              "  (linear): Linear(in_features=40, out_features=34, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfBZ1MBrPfad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Extract the concatenated weights from the models\n",
        "_, character_embeddings = character_embedding_model(input_batch_torch)\n",
        "char_embed = character_embeddings.detach().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlCeWT8eeLnd",
        "colab_type": "text"
      },
      "source": [
        "## 2.3. Sequence model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwA-NN3EJ4Ig",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.1. Apply/Import Word Embedding and Character Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6Z7B_kP0kgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Concatenate word embeddings and character embedding\n",
        "concatenated_embeddings = np.concatenate((word_embeddings, char_embed), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PeQiSnB_m3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Map the final embeddings to data sets\n",
        "\n",
        "def map_embeddings(input_set):\n",
        "  mapped_embeddings = []\n",
        "  for review in input_set:\n",
        "    embedding_list = []\n",
        "    for word in review:\n",
        "\n",
        "      try:\n",
        "        index = int(training_word_dict[word])\n",
        "        embedding_list.append(concatenated_embeddings[index])\n",
        "\n",
        "      except KeyError:\n",
        "        # Handle Out of Vocabulary words\n",
        "        index = int(training_word_dict['<OOV>'])\n",
        "        embedding_list.append(concatenated_embeddings[index]) \n",
        "\n",
        "    mapped_embeddings.append(embedding_list)\n",
        "  return mapped_embeddings\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUzJ9GW8C6DE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Function to add <PAD> to all the lists to make them of equal length\n",
        "\n",
        "def add_padding_to_lists(corpus, seq_length):\n",
        "    output = []\n",
        "    for sentence in corpus:\n",
        "        if len(sentence)>seq_length:\n",
        "            output.append(sentence[:seq_length])\n",
        "        else:\n",
        "            for j in range(seq_length-len(sentence)):\n",
        "                sentence.append(\"<PAD>\")\n",
        "            output.append(sentence)\n",
        "    return output\n",
        "\n",
        "# Generate batch function\n",
        "def make_batch(mapped_embeddings, label, batch_size):\n",
        "    idx = np.random.choice(len(mapped_embeddings), size = batch_size, replace = False)\n",
        "    list_onereview = []\n",
        "    for ele in idx:\n",
        "      list_onereview = list_onereview + [mapped_embeddings[ele]]\n",
        "    return list_onereview, label[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpYCL17JKZxl",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.2. Build Sequence Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDrt-cHBH5KJ",
        "colab_type": "text"
      },
      "source": [
        "*Justification*\n",
        "\n",
        "**PREPROCESSING FOR SEQUENCE TO SEQUENCE MODEL**\n",
        "\n",
        "*For preprocessing, I have padded every list in the training reviews by appending <PAD> at the end. This was done so that every list in the tarining data will be of equal length which will help model to learn better.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZzsEcC0AEPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Get the length of list having maximum elements\n",
        "list_length_train = [len(s) for s in reviews_train_ns]\n",
        "maxlength_list_train = max(list_length_train)\n",
        "\n",
        "# Add paddings to all your lists\n",
        "train_word_pad = add_padding_to_lists(reviews_train_le, maxlength_list_train)\n",
        "\n",
        "# Get the embeddings for training data set our final embeddings\n",
        "mapped_train_embeddings = map_embeddings(train_word_pad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ5YMLl0Dg6i",
        "colab_type": "text"
      },
      "source": [
        "*Justification:*\n",
        "\n",
        "**HYPERPARAMETERS:**\n",
        "\n",
        "**input_n: 74**\n",
        "\n",
        "*- Total Embedding Size of word and characters*\n",
        "\n",
        "**class_n: 2**\n",
        "\n",
        "*- Number of distinct labels*\n",
        "\n",
        "**Learning Rate: 0.01**\n",
        "\n",
        "*-  With learning rate greater than 0.01, model was training very fast and was overfitting. That is why I chose the optimal learning rate of 0.01 so that the model does not get stuck or converge too quickly.*\n",
        "\n",
        "**Batch Size: 500**\n",
        "\n",
        "*- Batch Size is 500 as the data is too huge and most of the data should be used for our model to train properly.*\n",
        "\n",
        "**Number of hidden layers: 10**\n",
        "\n",
        "*- 10 for proper training of model for every review*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKWtv6jvAHUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# HYPERPARAMETERS\n",
        "\n",
        "input_n = len(mapped_train_embeddings[0][0]) # Number of input: Embedding size\n",
        "class_n = len(list(set(sentiments_train)))\n",
        "hidden_n = 10 # Number of hidden layers\n",
        "batchSize_seqtoSeq = 500 # Batch Size\n",
        "learningRate_seqtoSeq = 0.01 # Learning Rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTZ066XaJGnz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Sequence to Sequence Sentiment Analysis Model (M to 1)\n",
        "\n",
        "class SeqNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SeqNet, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_n, hidden_n, batch_first =True)\n",
        "        self.linear = nn.Linear(hidden_n, class_n)\n",
        "\n",
        "    def forward(self, x):        \n",
        "\n",
        "        # lstm layer\n",
        "        x,_ = self.lstm(x)\n",
        "\n",
        "        # linear layer\n",
        "        x = self.linear(x[:,-1,:])\n",
        "        \n",
        "        # softmax layer\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "              \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BaOiaGRLW7R",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.3. Train Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7IKbXgwxWVt",
        "colab_type": "code",
        "outputId": "11082ea3-1caf-4e59-986c-bd4a7b48fc17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "seq_net = SeqNet()\n",
        "\n",
        "# Cross Entropy is used so that the decision boundary is large.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Adam optimizer can handle sparse gradients and provide high F1 score as well\n",
        "optimizer = optim.Adam(seq_net.parameters(), lr=learningRate_seqtoSeq)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(1000):\n",
        "\n",
        "    # Make batches\n",
        "    input_batch, target_batch = make_batch(mapped_train_embeddings, sentiments_train_encoded, batchSize_seqtoSeq)\n",
        "\n",
        "    # Convert your final embeddings to an array \n",
        "    input_batch_array = np.asarray(input_batch)\n",
        "\n",
        "    # Convert input and labels into torch\n",
        "    input_batch_torch = torch.from_numpy(input_batch_array).float()\n",
        "    target_batch_torch = torch.from_numpy(target_batch).view(-1)\n",
        "\n",
        "    # Forward propagation\n",
        "    seq_net.train()\n",
        "    outputs = seq_net(input_batch_torch) \n",
        "\n",
        "    # Calculate loss\n",
        "    loss = criterion(outputs, target_batch_torch)\n",
        "\n",
        "    # Back propagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Zero Grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if epoch % 100 == 99:\n",
        "      print('Epoch: %d, loss: %.4f' %(epoch + 1, loss.item()))\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100, loss: 0.6924\n",
            "Epoch: 200, loss: 0.6952\n",
            "Epoch: 300, loss: 0.6931\n",
            "Epoch: 400, loss: 0.6908\n",
            "Epoch: 500, loss: 0.6933\n",
            "Epoch: 600, loss: 0.6948\n",
            "Epoch: 700, loss: 0.6928\n",
            "Epoch: 800, loss: 0.6930\n",
            "Epoch: 900, loss: 0.6934\n",
            "Epoch: 1000, loss: 0.6929\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2feNpG-LZx2",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.4. Save Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sflUAgV4L1o8",
        "colab_type": "code",
        "outputId": "13d9197d-2efb-478a-9440-cca913dc893d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8qZLjqekHk8",
        "colab_type": "code",
        "outputId": "b16c34f8-7ee1-40be-8276-49188339a085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Save the Sequence to Sequence Model\n",
        "torch.save(seq_net, '/content/gdrive/My Drive/Models/sequence2sequence_model.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type SeqNet. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zFo6YppL6w3",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.5. Load Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUscZywAkMxP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1ipDYLgOsUjPFwj2tCn-awiDhl_m7B3rH'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('sequence2sequence_model.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtNxLzDGMCan",
        "colab_type": "code",
        "outputId": "96db11c2-7d03-4876-9725-31b26ce363d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Load the model\n",
        "seqtoseqmodel = torch.load('sequence2sequence_model.pt')\n",
        "\n",
        "# Evaluate the model\n",
        "seqtoseqmodel.eval()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SeqNet(\n",
              "  (lstm): LSTM(74, 10, batch_first=True)\n",
              "  (linear): Linear(in_features=10, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4mpRpocePLN",
        "colab_type": "text"
      },
      "source": [
        "# 3 - Evaluation\n",
        "\n",
        "(*Please show your empirical evidence*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEW1zMgVMREr",
        "colab_type": "text"
      },
      "source": [
        "## 3.1. Performance Evaluation\n",
        "\n",
        "\n",
        "You are required to provide the table with precision, recall, f1 of test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf7vK0-6J6Fm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Get the length of list having maximum elements\n",
        "list_length_test = [len(s) for s in reviews_test_ns]\n",
        "maxlength_list_test = max(list_length_test)\n",
        "\n",
        "# Add paddings to all your lists\n",
        "test_word_pad = add_padding_to_lists(reviews_test_le, maxlength_list_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFF4JhKYJ58U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Get the embeddings for testing data set our final embeddings\n",
        "mapped_test_embeddings = map_embeddings(test_word_pad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X5qRfwmJ51F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Make batches for testing data set\n",
        "test_input, test_labels = make_batch(mapped_test_embeddings, sentiments_test_encoded, 5000)\n",
        "\n",
        "# Convert your final embeddings to an array \n",
        "test_input_array = np.asarray(test_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZj1JVahJ5tT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Get the predicted labels for your testing data set\n",
        "outputs = seqtoseqmodel(torch.from_numpy(test_input_array).float()) \n",
        "\n",
        "# Convert your predicted labels into torch\n",
        "_, predicted = torch.max(outputs, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPHCb-bneTI9",
        "colab_type": "code",
        "outputId": "8d0fb868-0210-49d7-e75c-a664c53d91f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Calculate the precision, recall, F1-score and support\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_labels, predicted.cpu().numpy(),digits=4))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5058    1.0000    0.6718      2529\n",
            "           1     0.0000    0.0000    0.0000      2471\n",
            "\n",
            "    accuracy                         0.5058      5000\n",
            "   macro avg     0.2529    0.5000    0.3359      5000\n",
            "weighted avg     0.2558    0.5058    0.3398      5000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evAmnHixYZ6a",
        "colab_type": "text"
      },
      "source": [
        "F1 score is the balance between recall and precision. It takes into account false positives and false negatives. This score is better than accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P28Z1k36MZuo",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. Hyperparameter Testing\n",
        "*You are required to draw a graph(y-axis: f1, x-axis: epoch) for test set and explain the optimal number of epochs based on the learning rate you have already chosen.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTLyQEeZMZ2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfv8rWTKPzeb",
        "colab_type": "text"
      },
      "source": [
        "## Object Oriented Programming codes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS23AjBRSZaX",
        "colab_type": "text"
      },
      "source": [
        "*You can use multiple code snippets. Just add more if needed* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1hVmx4E52dXS",
        "colab": {}
      },
      "source": [
        "# If you used OOP style, use this section"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}