{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqPIy1CkK0P-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "#You can enable GPU here (cuda); or just CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLb0__26OqjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1gNfBqguzBu8cHKMPc8C44GbvD443dNC5'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('twitter.csv')  \n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"twitter.csv\")\n",
        "df_pick = df.sample(40,random_state=24)\n",
        "\n",
        "raw_text = df_pick[\"Text\"].tolist()\n",
        "raw_label = df_pick[\"Label\"].tolist()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "text_train,text_test,label_train,label_test = train_test_split(raw_text,raw_label,test_size=0.25,random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqDwxPONf4kf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CASE FOLDING\n",
        "\n",
        "text_train = [s.lower() for s in text_train]\n",
        "text_test = [s.lower() for s in text_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhAS7JTMga32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#REMOVE PUNCTUATIONS\n",
        "import re\n",
        "def remove_punctuation_re(x):\n",
        "    x = re.sub(r'[^\\w\\s]', '' ,x)\n",
        "    return x\n",
        "    \n",
        "text_train = [remove_punctuation_re(s) for s in text_train]\n",
        "text_test = [remove_punctuation_re(s) for s in text_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxGf0eHQgdoD",
        "colab_type": "code",
        "outputId": "acd4775d-9084-48ca-eb01-7a175881f614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#TOKENIZATION\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "# tknzr = word_tokenize()\n",
        "\n",
        "text_train = [word_tokenize(s) for s in text_train]\n",
        "text_test = [word_tokenize(s) for s in text_test]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SEwpxceggpc",
        "colab_type": "code",
        "outputId": "241077c3-7f09-437f-aa9e-cb0d08a2f5f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#REMOVE STOP WORDS\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "stop_words = sw.words()\n",
        "\n",
        "text_train_ns=[]\n",
        "for tokens in text_train:\n",
        "    filtered_sentence1 = [w1 for w1 in tokens if not w1 in stop_words]\n",
        "    text_train_ns.append(filtered_sentence1)\n",
        "\n",
        "text_test_ns=[]\n",
        "for tokens in text_test:\n",
        "    filtered_sentence2 = [w2 for w2 in tokens if not w2 in stop_words]\n",
        "    text_test_ns.append(filtered_sentence2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CxswZKzgkOU",
        "colab_type": "code",
        "outputId": "fc7540a2-046f-4776-c194-68e1ccc531a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#LEMMATIZATION\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "text_train_le = []\n",
        "for tokens in text_train_ns:\n",
        "    lemma_sentence1 = [lemmatizer.lemmatize(w1) for w1 in tokens ]\n",
        "    text_train_le.append(lemma_sentence1)\n",
        "\n",
        "text_test_le = []\n",
        "for tokens in text_test_ns:\n",
        "    lemma_sentence2 = [lemmatizer.lemmatize(w2) for w2 in tokens ]\n",
        "    text_test_le.append(lemma_sentence2)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyO9Q-hJgocA",
        "colab_type": "code",
        "outputId": "fd92702a-3991-44bc-df29-ef5edbdaa0c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#LABEL ENCODING\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "labels = np.unique(label_train)\n",
        "\n",
        "lEnc = LabelEncoder()\n",
        "lEnc.fit(labels)\n",
        "label_train_n = lEnc.transform(label_train)\n",
        "label_test_n = lEnc.transform(label_test)\n",
        "numClass = len(labels)\n",
        "\n",
        "print(labels)\n",
        "print(lEnc.transform(labels))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['none' 'racism' 'sexism']\n",
            "[0 1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEfS8MaRguUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#EMBEDDINGS\n",
        "len_list = [len(s) for s in text_train_ns]\n",
        "seq_length = max(len_list)\n",
        "\n",
        "def add_padding(corpus, seq_length):\n",
        "    output = []\n",
        "    for sentence in corpus:\n",
        "        if len(sentence)>seq_length:\n",
        "            output.append(sentence[:seq_length])\n",
        "        else:\n",
        "            for j in range(seq_length-len(sentence)):\n",
        "                sentence.append(\"<PAD>\")\n",
        "            output.append(sentence)\n",
        "    return output\n",
        "\n",
        "text_train_pad = add_padding(text_train_le,seq_length )\n",
        "text_test_pad = add_padding(text_test_le,seq_length )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fCXSSAYg0Xw",
        "colab_type": "code",
        "outputId": "635861a2-5365-492b-e263-8a50b3d5ff85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "#DOWNLOAD EMBEDDINGS\n",
        "import gensim.downloader as api\n",
        "word_emb_model = api.load(\"glove-twitter-25\") #this is only example"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7Gu0Zzng47C",
        "colab_type": "code",
        "outputId": "a6d23fca-6cc9-43da-ed28-9eea05fdfb4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#GET EMBEDDINGS\n",
        "def get_embeddings(corpus,word_emb_model):\n",
        "    emb_dim = word_emb_model.vector_size\n",
        "    out = []\n",
        "    for sentence in corpus:\n",
        "        out_temp = []\n",
        "        for word in sentence:\n",
        "            try:\n",
        "                out_temp.append(word_emb_model.wv[word])\n",
        "            except:\n",
        "                out_temp.append([0]*emb_dim)\n",
        "    \n",
        "        out.append(out_temp)\n",
        "    return np.array(out)\n",
        "\n",
        "train_emb = get_embeddings(text_train_pad,word_emb_model)\n",
        "test_emb = get_embeddings(text_test_pad,word_emb_model)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoMUgwnEg9ZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#MODEL\n",
        "n_input = train_emb.shape[2]\n",
        "n_hidden = 50\n",
        "n_class = len(labels)\n",
        "total_epoch = 100\n",
        "learning_rate = 0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqy3ZERJg-kl",
        "colab_type": "code",
        "outputId": "628c7e29-868d-4051-f8ea-c85d8dda51b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.lstm = nn.LSTM(n_input, n_hidden, num_layers=2, batch_first =True, dropout=0.2)\n",
        "        self.linear = nn.Linear(n_hidden,n_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x,_ = self.lstm(x)\n",
        "        x = self.linear(x[:,-1,:])\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net().to(device)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "input_batch_torch = torch.from_numpy(np.array(train_emb)).float().to(device)\n",
        "target_batch_torch = torch.from_numpy(np.array(label_train_n)).view(-1).to(device)\n",
        "\n",
        "\n",
        "for epoch in range(total_epoch):  \n",
        "    \n",
        "    net.train()\n",
        "    outputs = net(input_batch_torch) \n",
        "    loss = criterion(outputs, target_batch_torch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    net.eval()\n",
        "    outputs = net(input_batch_torch) \n",
        "    \n",
        "    if epoch%10 == 9:\n",
        "        loss = criterion(outputs, target_batch_torch)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        acc= accuracy_score(predicted.cpu().numpy(),target_batch_torch.cpu().numpy())\n",
        "\n",
        "        print('Epoch: %d, loss: %.5f, train_acc: %.2f' %(epoch + 1, loss.item(), acc))\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10, loss: 0.70659, train_acc: 0.70\n",
            "Epoch: 20, loss: 0.21098, train_acc: 0.93\n",
            "Epoch: 30, loss: 0.18696, train_acc: 0.93\n",
            "Epoch: 40, loss: 0.01327, train_acc: 1.00\n",
            "Epoch: 50, loss: 0.00256, train_acc: 1.00\n",
            "Epoch: 60, loss: 0.00120, train_acc: 1.00\n",
            "Epoch: 70, loss: 0.00077, train_acc: 1.00\n",
            "Epoch: 80, loss: 0.00060, train_acc: 1.00\n",
            "Epoch: 90, loss: 0.00051, train_acc: 1.00\n",
            "Epoch: 100, loss: 0.00045, train_acc: 1.00\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlihfdpig_0f",
        "colab_type": "code",
        "outputId": "1b54c61c-6239-4e2d-c21c-4c01e86bcc25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#SAVE THE MODEL\n",
        "torch.save(net, 'lab5.pt')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnbDhKcZhDOd",
        "colab_type": "code",
        "outputId": "5d5c3244-0859-4b93-9b2b-b0afefa5d5d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "#LOAD THE MODEL\n",
        "model2 = torch.load('lab5.pt')\n",
        "model2.eval()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (lstm): LSTM(25, 50, num_layers=2, batch_first=True, dropout=0.2)\n",
              "  (linear): Linear(in_features=50, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ywYcUcShEuD",
        "colab_type": "code",
        "outputId": "d153a6e7-6939-46e2-8d03-eb75add0174b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "#TESTING\n",
        "input_batch_torch = torch.from_numpy(np.array(test_emb)).float().to(device)\n",
        "\n",
        "outputs = model2(input_batch_torch) \n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(label_test_n,predicted.cpu().numpy()))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.71      0.71         7\n",
            "           1       0.50      0.50      0.50         2\n",
            "           2       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.60        10\n",
            "   macro avg       0.40      0.40      0.40        10\n",
            "weighted avg       0.60      0.60      0.60        10\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}