{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Group4A2_Model2_DifferentNumberofLayers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJxNFWxY_NC5",
        "colab_type": "text"
      },
      "source": [
        "# Readme.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrAqyLyp_bhh",
        "colab_type": "text"
      },
      "source": [
        "If you wish to run this .ipynb file, run the cells in which the following is written.\n",
        "```\n",
        "#**********************************RUN THIS CELL**********************************#\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXEscWBrhjgb",
        "colab_type": "text"
      },
      "source": [
        "#Load the Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkZQkNwkh46V",
        "colab_type": "code",
        "outputId": "70ebddc7-e6e6-4aef-e0cc-40bf8e63f5b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#Training Data\n",
        "id = '19E2v3QyOqUohMG65Qn5n_zlAhzJ0cvN4'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('train.csv')\n",
        "\n",
        "#Validation Data\n",
        "id = '1BMX04M5J-6Pqsejyf1rp7AIZGJiLdl7a'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('val.csv')\n",
        "\n",
        "#Testing Data\n",
        "id = '1NrkdJJ00OwD8naPucpzFh_KnClBp0NZZ'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('test.csv')\n",
        "\n",
        "import pandas as pd\n",
        "df_train = pd.read_csv(\"train.csv\")\n",
        "df_val = pd.read_csv(\"val.csv\")\n",
        "df_test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "train_data = df_train['Sentence'].tolist()\n",
        "train_labels = df_train['NER'].tolist()\n",
        "val_data = df_val['Sentence'].tolist()\n",
        "val_labels = df_val['NER'].tolist()\n",
        "test_data = df_test['Sentence'].tolist()\n",
        "test_labels = df_test['NER'].tolist()\n",
        "\n",
        "print(\"Training set number:\",len(train_data))\n",
        "print(\"Training labels number:\",len(train_labels))\n",
        "print(\"Validation set number:\",len(val_data))\n",
        "print(\"Validation labels number:\",len(val_labels))\n",
        "print(\"Testing set number:\",len(test_data))\n",
        "print(\"Testing labels number:\",len(test_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set number: 3000\n",
            "Training labels number: 3000\n",
            "Validation set number: 700\n",
            "Validation labels number: 700\n",
            "Testing set number: 3684\n",
            "Testing labels number: 3684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxIqSiw8j_QE",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33AvpgmEiSMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "#Tokenization\n",
        "\n",
        "train_data_tokenized = [s.split() for s in train_data]\n",
        "train_labels_tokenized = [s.split() for s in train_labels]\n",
        "val_data_tokenized = [s.split() for s in val_data]\n",
        "val_labels_tokenized = [s.split() for s in val_labels]\n",
        "test_data_tokenized = [s.split() for s in test_data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-q4UJm0kDi3",
        "colab_type": "text"
      },
      "source": [
        "# Make Dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuZmoeNoiC3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "word_to_ix = {}\n",
        "for sentence in train_data_tokenized + val_data_tokenized + test_data_tokenized:\n",
        "    for word in sentence:\n",
        "        word = word.lower()\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "word_list = list(word_to_ix.keys())\n",
        "\n",
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "tag_to_ix = {START_TAG:0, STOP_TAG:1}\n",
        "for tags in train_labels_tokenized:\n",
        "    for tag in tags:\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "tags_list = list(tag_to_ix.keys())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY15CHqUl0T4",
        "colab_type": "text"
      },
      "source": [
        "# Get the Indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9v8npuZl4gE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "def to_index(data, to_ix):\n",
        "    input_index_list = []\n",
        "    for sent in data:\n",
        "        index_list = []\n",
        "        for w in sent:\n",
        "            try:\n",
        "                index_list.append(to_ix[w])\n",
        "            except:\n",
        "                index_list.append(0)\n",
        "        input_index_list.append(index_list)\n",
        "    return input_index_list\n",
        "\n",
        "train_input_index =  to_index(train_data_tokenized, word_to_ix)\n",
        "train_output_index = to_index(train_labels_tokenized, tag_to_ix)\n",
        "val_input_index = to_index(val_data_tokenized, word_to_ix)\n",
        "val_output_index = to_index(val_labels_tokenized, tag_to_ix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFX9PEvFsMya",
        "colab_type": "text"
      },
      "source": [
        "#  Generate Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-2VpaQXkHDK",
        "colab_type": "text"
      },
      "source": [
        "## Generate Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3t4JIM0jxWi",
        "colab_type": "code",
        "outputId": "952f5104-e933-4d29-de34-09e5840f14c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gensim.downloader as api\n",
        "word_emb_model = api.load(\"glove-twitter-100\") \n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "embedding_matrix = []     \n",
        "for word in word_list:\n",
        "    try:\n",
        "        embedding_matrix.append(word_emb_model.wv[word])\n",
        "    except:\n",
        "        embedding_matrix.append([0]*EMBEDDING_DIM)\n",
        "embedding_matrix = np.array(embedding_matrix)\n",
        "embedding_matrix.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[=================================================-] 98.8% 382.4/387.1MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13972, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4KuEw1uueLH",
        "colab_type": "text"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkqcA1WUq_vp",
        "colab_type": "text"
      },
      "source": [
        "### POS using NLTK "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_og73sCvrETB",
        "colab_type": "code",
        "outputId": "321cf075-5816-440e-a9cd-6445d84d2ea0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "get_POS_TAGS = []\n",
        "POS_Tags = []\n",
        "Word_to_POS = {}\n",
        "\n",
        "# for sentence in train_data_tokenized + val_data_tokenized:\n",
        "get_POS_TAGS = nltk.pos_tag(word_list)\n",
        "POS_Tags.append([lis[1] for lis in get_POS_TAGS])\n",
        "\n",
        "POS_Tags = POS_Tags[0]\n",
        "\n",
        "#Get Unique POS Tags\n",
        "uniquePOSTags = list(set(POS_Tags))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiHwe--Gq_lG",
        "colab_type": "text"
      },
      "source": [
        "### Convert POS tags to One Hot Vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_CsVp6JrK3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# integer encode\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(uniquePOSTags)\n",
        "\n",
        "# binary encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(uniquePOSTags), 1)\n",
        "POS_Tags_one_hot_encoded = onehot_encoder.fit_transform(integer_encoded.reshape(len(uniquePOSTags), 1))\n",
        "\n",
        "POS_to_OneHot = {}\n",
        "\n",
        "for i in range(len(uniquePOSTags)):\n",
        "    if uniquePOSTags[i] not in POS_to_OneHot:\n",
        "      POS_to_OneHot[uniquePOSTags[i]] = POS_Tags_one_hot_encoded[i]\n",
        "\n",
        "#GET POS one hot for word\n",
        "Word_to_OneHot = []\n",
        "\n",
        "for i in range(len(POS_Tags)):\n",
        "  Word_to_OneHot.append(POS_to_OneHot[POS_Tags[i]]) #[POS_Tags[0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54lnegTPSwn7",
        "colab_type": "text"
      },
      "source": [
        "## Character Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTmpfjPZS0Qu",
        "colab_type": "code",
        "outputId": "af12790e-e7f4-4e24-c5d5-1b99378fdcaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Fetching the maximum length of a word in the list from both the dataset\n",
        "max_word_len_train = len(max(word_list, key=len))\n",
        "\n",
        "# Adding an extra character to make all the words of the reviews of the same length as maximum length of the word\n",
        "def add_padding(corpus, max_word_len):\n",
        "    output = []\n",
        "    for word in corpus:\n",
        "        if len(word)>max_word_len:\n",
        "            output.append(word[:max_word_len])\n",
        "        else:\n",
        "            for j in range(max_word_len-len(word)):\n",
        "                word = word + \"-\"\n",
        "            output.append(word)\n",
        "    return output\n",
        "\n",
        "wordlist_train_pad = add_padding(word_list, max_word_len_train)\n",
        "\n",
        "#Assume that we have the following character instances\n",
        "char_arr = ['\"', '$', '%', '&', \"'\", '(', ')', '*',\n",
        "            '+', ',', '-', '.', '/', '0', '1', '2',\n",
        "            '3', '4', '5', '6', '7', '8', '9', ':',\n",
        "            ';', '=', '?', '[', ']', '`', 'a', 'b',\n",
        "            'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j',\n",
        "            'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r',\n",
        "            's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \n",
        "            '@', '#']\n",
        "            # Added few extra characters in the dictionary\n",
        "            # to handle the Padded and Out of Vocabulary values\n",
        "\n",
        "# Create a dictionary for above char_arr\n",
        "char_dict = {n: i for i, n in enumerate(char_arr)}\n",
        "# Get the dictionary length\n",
        "charDict_len = len(char_dict)\n",
        "\n",
        "# Get one-hot encoding for every word\n",
        "def encode_words(seq_data):\n",
        "  input_batch = []\n",
        "    \n",
        "  for seq in seq_data:\n",
        "    input_data = [char_dict[n] for n in seq]\n",
        "    input_batch.append(np.eye(charDict_len)[input_data])\n",
        "  return input_batch\n",
        "\n",
        "char_embeds = encode_words(wordlist_train_pad)\n",
        "char_vector = np.array(char_embeds)\n",
        "char_vector.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13972, 60, 58)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELEqQL_uu2D8",
        "colab_type": "text"
      },
      "source": [
        "## Concatenate the features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwSkLH-VSSdk",
        "colab_type": "text"
      },
      "source": [
        "### Concatenate Word Embeddings + Character Embeddings + POS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1S5qXbzSW3_",
        "colab_type": "code",
        "outputId": "9e3cf8bc-e16a-4562-a581-1ad7d2a21b62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "concat1 = np.concatenate((char_vector[:,:,-1], embedding_matrix),axis = 1)\n",
        "embedding_matrix = np.concatenate((concat1, Word_to_OneHot),axis = 1)\n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13972, 197)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qextJRqt8L3",
        "colab_type": "text"
      },
      "source": [
        "# Bi-LSTM CRF Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iyhj_7XvkMCx",
        "colab_type": "text"
      },
      "source": [
        "## NER Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5AgRWakkfmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=3, bidirectional=True, dropout = 0.2)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
        "        self.out = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "    def _cal_attention(self, lstm_out, method):\n",
        "        attention_result = torch.zeros(lstm_out.size()[0], self.hidden_dim * 2, device=device)\n",
        "        if method == 'ATTN_TYPE_DOT_PRODUCT':\n",
        "            # bmm: https://pytorch.org/docs/master/generated/torch.bmm.html\n",
        "            for i in range(lstm_out.size()[0]):\n",
        "                hidden = lstm_out[i]\n",
        "                attn_weights = F.softmax(torch.bmm(hidden.unsqueeze(0).unsqueeze(0), lstm_out.T.unsqueeze(0)), dim=-1)\n",
        "                attn_output = torch.bmm(attn_weights, lstm_out.unsqueeze(0))\n",
        "                concat_output = torch.cat((hidden.unsqueeze(0),attn_output[0]), 1)\n",
        "                attention_result[i] = concat_output.squeeze(0)\n",
        "        elif method == 'ATTN_TYPE_SCALE_DOT_PRODUCT':\n",
        "            for i in range(lstm_out.size()[0]):\n",
        "                hidden = lstm_out[i]\n",
        "                attn_weights = F.softmax(1/np.sqrt(self.hidden_dim)*torch.bmm(hidden.unsqueeze(0).unsqueeze(0), lstm_out.T.unsqueeze(0)), dim=-1)\n",
        "                attn_output = torch.bmm(attn_weights, lstm_out.unsqueeze(0))\n",
        "                concat_output = torch.cat((hidden.unsqueeze(0),attn_output[0]), 1)\n",
        "                attention_result[i] = concat_output.squeeze(0)\n",
        "        \n",
        "        attention_out = self.hidden2tag(self.out(attention_result))\n",
        "        return attention_out\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(6, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(6, 1, self.hidden_dim // 2).to(device))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        (h_n,h_c) = self.hidden\n",
        "        hidden_out =torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        return lstm_out, hidden_out\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "        lstm_out, hidden = self._get_lstm_features(sentence)\n",
        "        attention_feats = self._cal_attention(lstm_out, 'ATTN_TYPE_SCALE_DOT_PRODUCT')\n",
        "        forward_score = self._forward_alg(attention_feats)\n",
        "        gold_score = self._score_sentence(attention_feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats, hidden_out = self._get_lstm_features(sentence)\n",
        "        attention_feats = self._cal_attention(lstm_feats, 'ATTN_TYPE_SCALE_DOT_PRODUCT')\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(attention_feats)\n",
        "        return score, tag_seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUNHEV1kiDKt",
        "colab_type": "text"
      },
      "source": [
        "## Calculate Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJmu0oSsjLBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "import numpy as np\n",
        "def cal_acc(model, input_index, output_index):\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "    for i,idxs in enumerate(input_index):\n",
        "        ground_truth += output_index[i]\n",
        "        score, pred = model(torch.tensor(idxs, dtype=torch.long).to(device))\n",
        "        predicted += pred\n",
        "    accuracy = sum(np.array(ground_truth) == np.array(predicted))/len(ground_truth)\n",
        "    return ground_truth, predicted, accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_82UaXEOhoQQ",
        "colab_type": "text"
      },
      "source": [
        "## Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyuzZ_et6FD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "HIDDEN_DIM = 50\n",
        "EMBEDDING_DIM = 197\n",
        "model2 = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM).to(device)\n",
        "optimizer = optim.SGD(model2.parameters(), lr = 0.01, weight_decay = 1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9UiokVOjPUn",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0CMFVSwlLru",
        "colab_type": "code",
        "outputId": "fc6eb638-bc49-4bd7-fad7-e6a939536875",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "\"\"\"Each epoch will take about 1-2 minutes-- 133.62sec\"\"\"\n",
        "\n",
        "import datetime\n",
        "\n",
        "for epoch in range(20):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "    c = 0\n",
        "    model2.train()\n",
        "    for i, idxs in enumerate(train_input_index):\n",
        "        tags_index = train_output_index[i]\n",
        "        # print('t:',c,tags_index)\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model2.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        # print('s:',sentence_in)\n",
        "        c+=1\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model2.neg_log_likelihood(sentence_in, targets)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    model2.eval()\n",
        "    _, _, train_acc = cal_acc(model2, train_input_index,train_output_index)\n",
        "    time2 = datetime.datetime.now()\n",
        "  \n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, (time2-time1).total_seconds()))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 19177.29, train acc: 0.8644, time: 208.87s\n",
            "Epoch:2, Training loss: 9871.58, train acc: 0.9172, time: 207.85s\n",
            "Epoch:3, Training loss: 6580.19, train acc: 0.9384, time: 209.30s\n",
            "Epoch:4, Training loss: 4954.15, train acc: 0.9485, time: 210.20s\n",
            "Epoch:5, Training loss: 4135.30, train acc: 0.9528, time: 214.25s\n",
            "Epoch:6, Training loss: 3492.75, train acc: 0.9585, time: 214.61s\n",
            "Epoch:7, Training loss: 3079.74, train acc: 0.9620, time: 216.01s\n",
            "Epoch:8, Training loss: 2730.65, train acc: 0.9676, time: 216.03s\n",
            "Epoch:9, Training loss: 2421.45, train acc: 0.9711, time: 215.77s\n",
            "Epoch:10, Training loss: 2017.13, train acc: 0.9740, time: 216.62s\n",
            "Epoch:11, Training loss: 1932.19, train acc: 0.9758, time: 218.96s\n",
            "Epoch:12, Training loss: 1705.93, train acc: 0.9763, time: 218.22s\n",
            "Epoch:13, Training loss: 1520.80, train acc: 0.9801, time: 210.71s\n",
            "Epoch:14, Training loss: 1444.11, train acc: 0.9810, time: 209.46s\n",
            "Epoch:15, Training loss: 1224.42, train acc: 0.9793, time: 210.88s\n",
            "Epoch:16, Training loss: 1111.59, train acc: 0.9851, time: 213.88s\n",
            "Epoch:17, Training loss: 1011.42, train acc: 0.9885, time: 208.17s\n",
            "Epoch:18, Training loss: 894.21, train acc: 0.9906, time: 207.22s\n",
            "Epoch:19, Training loss: 937.23, train acc: 0.9899, time: 202.50s\n",
            "Epoch:20, Training loss: 848.78, train acc: 0.9869, time: 200.14s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IY2YEXHltER",
        "colab_type": "text"
      },
      "source": [
        "# Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NpwVb3YlsZ5",
        "colab_type": "code",
        "outputId": "4723c759-d42a-499b-c785-1a584ed50305",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpAwki6blv2y",
        "colab_type": "code",
        "outputId": "70fa9690-d579-414b-98ba-08e6c75a0028",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Save the Model\n",
        "torch.save(model2, '/content/gdrive/My Drive/NLP_Assignment2/NER_Model2.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type BiLSTM_CRF. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOGcSWf8l8dR",
        "colab_type": "text"
      },
      "source": [
        "# Load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfzwz6Tnl7qj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1CVM5IRTZrMtMNSPw05gx9VepGpgJmjbF'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('NER_Model2.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE6eONr-mCp2",
        "colab_type": "code",
        "outputId": "f1feeba9-cf9e-4033-ddb6-0c7139a0941d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "# Load the model\n",
        "NERModel2 = torch.load('NER_Model2.pt')\n",
        "\n",
        "# Evaluate the model\n",
        "NERModel2.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTM_CRF(\n",
              "  (word_embeds): Embedding(13972, 197)\n",
              "  (lstm): LSTM(197, 25, num_layers=3, dropout=0.2, bidirectional=True)\n",
              "  (hidden2tag): Linear(in_features=50, out_features=7, bias=True)\n",
              "  (embedding): Embedding(13972, 50)\n",
              "  (out): Linear(in_features=100, out_features=50, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YOZfleDuG6z",
        "colab_type": "text"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxyNPw7Xkxnq",
        "colab_type": "text"
      },
      "source": [
        "## Testing on Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvIG3rKp6ufz",
        "colab_type": "code",
        "outputId": "5c7b9f20-732b-43c5-fe23-fbe15f0336ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "y_true, y_pred, _ = cal_acc(NERModel2, val_input_index,val_output_index)\n",
        "\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       I-LOC     0.7674    0.9212    0.8373       419\n",
            "      I-MISC     0.7586    0.7059    0.7313       187\n",
            "       I-ORG     0.7222    0.4561    0.5591       285\n",
            "       I-PER     0.9444    0.9314    0.9379       875\n",
            "           O     0.9788    0.9865    0.9826      5790\n",
            "\n",
            "    accuracy                         0.9496      7556\n",
            "   macro avg     0.8343    0.8002    0.8096      7556\n",
            "weighted avg     0.9479    0.9496    0.9472      7556\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl7xmBIPtczt",
        "colab_type": "text"
      },
      "source": [
        "## Prediction on Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-Q4R2uatiF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "test_input_index = to_index(test_data_tokenized, word_to_ix)\n",
        "\n",
        "import numpy as np\n",
        "def calAccuracy_test(model, input_index):\n",
        "    predicted = []\n",
        "    for i, idxs in enumerate(input_index):\n",
        "        _, pred = model(torch.tensor(idxs, dtype=torch.long).to(device))\n",
        "        predicted += pred\n",
        "    return predicted\n",
        "\n",
        "y_pred_test = calAccuracy_test(NERModel2, test_input_index)\n",
        "\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "\n",
        "test_output = decode_output(y_pred_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3gPse2wttAZ",
        "colab_type": "text"
      },
      "source": [
        "## Write predictions to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWQRy4JhtwS3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "predicted_file = pd.DataFrame(columns = ['Id','Predicted'])\n",
        "predicted_file['Predicted'] = test_output\n",
        "predicted_file['Id'] = np.arange(0, len(test_output))\n",
        "predicted_file.to_csv('Model2.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}