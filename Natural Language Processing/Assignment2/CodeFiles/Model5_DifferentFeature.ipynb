{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Group4A2_Model5_DifferentFeature.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJxNFWxY_NC5",
        "colab_type": "text"
      },
      "source": [
        "# Readme.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrAqyLyp_bhh",
        "colab_type": "text"
      },
      "source": [
        "If you wish to run this .ipynb file, run the cells in which the following is written.\n",
        "```\n",
        "#**********************************RUN THIS CELL**********************************#\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXEscWBrhjgb",
        "colab_type": "text"
      },
      "source": [
        "##Load the Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkZQkNwkh46V",
        "colab_type": "code",
        "outputId": "087199de-849e-4796-8ba3-0bf65acfc192",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#Training Data\n",
        "id = '19E2v3QyOqUohMG65Qn5n_zlAhzJ0cvN4'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('train.csv')\n",
        "\n",
        "#Validation Data\n",
        "id = '1BMX04M5J-6Pqsejyf1rp7AIZGJiLdl7a'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('val.csv')\n",
        "\n",
        "#Testing Data\n",
        "id = '1NrkdJJ00OwD8naPucpzFh_KnClBp0NZZ'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('test.csv')\n",
        "\n",
        "import pandas as pd\n",
        "df_train = pd.read_csv(\"train.csv\")\n",
        "df_val = pd.read_csv(\"val.csv\")\n",
        "df_test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "train_data = df_train['Sentence'].tolist()\n",
        "train_labels = df_train['NER'].tolist()\n",
        "val_data = df_val['Sentence'].tolist()\n",
        "val_labels = df_val['NER'].tolist()\n",
        "test_data = df_test['Sentence'].tolist()\n",
        "test_labels = df_test['NER'].tolist()\n",
        "\n",
        "print(\"Training set number:\",len(train_data))\n",
        "print(\"Training labels number:\",len(train_labels))\n",
        "print(\"Validation set number:\",len(val_data))\n",
        "print(\"Validation labels number:\",len(val_labels))\n",
        "print(\"Testing set number:\",len(test_data))\n",
        "print(\"Testing labels number:\",len(test_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set number: 3000\n",
            "Training labels number: 3000\n",
            "Validation set number: 700\n",
            "Validation labels number: 700\n",
            "Testing set number: 3684\n",
            "Testing labels number: 3684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxIqSiw8j_QE",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33AvpgmEiSMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "#Tokenization\n",
        "\n",
        "train_data_tokenized = [s.split() for s in train_data]\n",
        "train_labels_tokenized = [s.split() for s in train_labels]\n",
        "val_data_tokenized = [s.split() for s in val_data]\n",
        "val_labels_tokenized = [s.split() for s in val_labels]\n",
        "test_data_tokenized = [s.split() for s in test_data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-q4UJm0kDi3",
        "colab_type": "text"
      },
      "source": [
        "# Make Dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuZmoeNoiC3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "word_to_ix = {}\n",
        "for sentence in train_data_tokenized + val_data_tokenized + test_data_tokenized:\n",
        "    for word in sentence:\n",
        "        word = word.lower()\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "word_list = list(word_to_ix.keys())\n",
        "\n",
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "tag_to_ix = {START_TAG:0, STOP_TAG:1}\n",
        "for tags in train_labels_tokenized:\n",
        "    for tag in tags:\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "tags_list = list(tag_to_ix.keys())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY15CHqUl0T4",
        "colab_type": "text"
      },
      "source": [
        "# Get the Indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9v8npuZl4gE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "def to_index(data, to_ix):\n",
        "    input_index_list = []\n",
        "    for sent in data:\n",
        "        index_list = []\n",
        "        for w in sent:\n",
        "            try:\n",
        "                index_list.append(to_ix[w])\n",
        "            except:\n",
        "                index_list.append(0)\n",
        "        input_index_list.append(index_list)\n",
        "    return input_index_list\n",
        "\n",
        "train_input_index =  to_index(train_data_tokenized, word_to_ix)\n",
        "train_output_index = to_index(train_labels_tokenized, tag_to_ix)\n",
        "val_input_index = to_index(val_data_tokenized, word_to_ix)\n",
        "val_output_index = to_index(val_labels_tokenized, tag_to_ix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFX9PEvFsMya",
        "colab_type": "text"
      },
      "source": [
        "#  Generate Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-2VpaQXkHDK",
        "colab_type": "text"
      },
      "source": [
        "## Generate Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3t4JIM0jxWi",
        "colab_type": "code",
        "outputId": "dd4ea607-f268-4e4f-b594-5b9d503a5312",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gensim.downloader as api\n",
        "word_emb_model = api.load(\"glove-twitter-50\") \n",
        "\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "embedding_matrix = []     \n",
        "for word in word_list:\n",
        "    try:\n",
        "        embedding_matrix.append(word_emb_model.wv[word])\n",
        "    except:\n",
        "        embedding_matrix.append([0]*EMBEDDING_DIM)\n",
        "embedding_matrix = np.array(embedding_matrix)\n",
        "embedding_matrix.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[=================================================-] 98.9% 197.4/199.5MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13972, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85T1Aje9MM5q",
        "colab_type": "text"
      },
      "source": [
        "## TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6zWnisUMSzu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "vectorizer = TfidfTransformer()\n",
        "\n",
        "pipe = Pipeline([('count', CountVectorizer(vocabulary = word_list)), ('tfid', TfidfTransformer())]).fit(train_data)\n",
        "TFIDF_array = pipe['tfid'].idf_\n",
        "\n",
        "TFIDF_Dict = {}\n",
        "\n",
        "for i in range(len(word_list)):\n",
        "    if TFIDF_array[i] not in TFIDF_Dict:\n",
        "      TFIDF_Dict[word_list[i]] = TFIDF_array[i]\n",
        "\n",
        "Word_to_TFIDF = []\n",
        "\n",
        "for i in range(len(word_list)):\n",
        "  Word_to_TFIDF.append(TFIDF_Dict[word_list[i]]) #[POS_Tags[0]]\n",
        "\n",
        "Word_to_TFIDF = np.reshape(Word_to_TFIDF, 13972)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELEqQL_uu2D8",
        "colab_type": "text"
      },
      "source": [
        "## Concatenate the features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tnAFfA_SCi5",
        "colab_type": "text"
      },
      "source": [
        "### Concatenate Word Embeddings with TFIDF Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etr8l8EHSJLP",
        "colab_type": "code",
        "outputId": "aba51cbb-4fa5-4e1d-978a-2ca988534ede",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "embedding_matrix = np.column_stack((embedding_matrix, Word_to_TFIDF))\n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13972, 51)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qextJRqt8L3",
        "colab_type": "text"
      },
      "source": [
        "# Bi-LSTM CRF Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iyhj_7XvkMCx",
        "colab_type": "text"
      },
      "source": [
        "## NER Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5AgRWakkfmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=2, bidirectional=True, dropout = 0.2)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
        "        # self.dropout_att=nn.Dropout(p = 0.5)\n",
        "        self.out = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "    def _cal_attention(self, lstm_out, method):\n",
        "        attention_result = torch.zeros(lstm_out.size()[0], self.hidden_dim * 2, device=device)\n",
        "        if method == 'ATTN_TYPE_DOT_PRODUCT':\n",
        "            # bmm: https://pytorch.org/docs/master/generated/torch.bmm.html\n",
        "            for i in range(lstm_out.size()[0]):\n",
        "                hidden = lstm_out[i]\n",
        "                attn_weights = F.softmax(torch.bmm(hidden.unsqueeze(0).unsqueeze(0), lstm_out.T.unsqueeze(0)), dim=-1)\n",
        "                attn_output = torch.bmm(attn_weights, lstm_out.unsqueeze(0))\n",
        "                concat_output = torch.cat((hidden.unsqueeze(0),attn_output[0]), 1)\n",
        "                attention_result[i] = concat_output.squeeze(0)\n",
        "        elif method == 'ATTN_TYPE_SCALE_DOT_PRODUCT':\n",
        "            for i in range(lstm_out.size()[0]):\n",
        "                hidden = lstm_out[i]\n",
        "                attn_weights = F.softmax(1/np.sqrt(self.hidden_dim)*torch.bmm(hidden.unsqueeze(0).unsqueeze(0), lstm_out.T.unsqueeze(0)), dim=-1)\n",
        "                attn_output = torch.bmm(attn_weights, lstm_out.unsqueeze(0))\n",
        "                concat_output = torch.cat((hidden.unsqueeze(0),attn_output[0]), 1)\n",
        "                attention_result[i] = concat_output.squeeze(0)\n",
        "        \n",
        "        attention_out = self.hidden2tag(self.out(attention_result))\n",
        "        return attention_out\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(4, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(4, 1, self.hidden_dim // 2).to(device))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        (h_n,h_c) = self.hidden\n",
        "        hidden_out =torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        return lstm_out, hidden_out\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "        lstm_out, hidden = self._get_lstm_features(sentence)\n",
        "        attention_feats = self._cal_attention(lstm_out, 'ATTN_TYPE_SCALE_DOT_PRODUCT')\n",
        "        forward_score = self._forward_alg(attention_feats)\n",
        "        gold_score = self._score_sentence(attention_feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats, hidden_out = self._get_lstm_features(sentence)\n",
        "\n",
        "        attention_feats = self._cal_attention(lstm_feats, 'ATTN_TYPE_SCALE_DOT_PRODUCT')\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(attention_feats)\n",
        "        return score, tag_seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUNHEV1kiDKt",
        "colab_type": "text"
      },
      "source": [
        "## Calculate Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJmu0oSsjLBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "import numpy as np\n",
        "def cal_acc(model, input_index, output_index):\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "    for i,idxs in enumerate(input_index):\n",
        "        ground_truth += output_index[i]\n",
        "        score, pred = model(torch.tensor(idxs, dtype=torch.long).to(device))\n",
        "        predicted += pred\n",
        "    accuracy = sum(np.array(ground_truth) == np.array(predicted))/len(ground_truth)\n",
        "    return ground_truth, predicted, accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_82UaXEOhoQQ",
        "colab_type": "text"
      },
      "source": [
        "## Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyuzZ_et6FD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "HIDDEN_DIM = 50\n",
        "EMBEDDING_DIM = 51\n",
        "model5 = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM).to(device)\n",
        "optimizer = optim.SGD(model5.parameters(), lr=0.01, weight_decay=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9UiokVOjPUn",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0CMFVSwlLru",
        "colab_type": "code",
        "outputId": "09a88d29-cf93-49b7-bc44-19aa935b4bc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "\"\"\"Each epoch will take about 1-2 minutes-- 133.62sec\"\"\"\n",
        "\n",
        "import datetime\n",
        "\n",
        "for epoch in range(20):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "    c = 0\n",
        "    model5.train()\n",
        "    for i, idxs in enumerate(train_input_index):\n",
        "        tags_index = train_output_index[i]\n",
        "        # print('t:',c,tags_index)\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model5.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        # print('s:',sentence_in)\n",
        "        c+=1\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model5.neg_log_likelihood(sentence_in, targets)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    model5.eval()\n",
        "    _, _, train_acc = cal_acc(model5, train_input_index,train_output_index)\n",
        "    time2 = datetime.datetime.now()\n",
        "  \n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, (time2-time1).total_seconds()))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 15047.25, train acc: 0.8999, time: 138.28s\n",
            "Epoch:2, Training loss: 8454.42, train acc: 0.9209, time: 138.15s\n",
            "Epoch:3, Training loss: 6800.16, train acc: 0.9291, time: 141.56s\n",
            "Epoch:4, Training loss: 5710.86, train acc: 0.9333, time: 144.99s\n",
            "Epoch:5, Training loss: 5037.03, train acc: 0.9434, time: 145.66s\n",
            "Epoch:6, Training loss: 4412.17, train acc: 0.9435, time: 141.52s\n",
            "Epoch:7, Training loss: 3953.50, train acc: 0.9494, time: 141.60s\n",
            "Epoch:8, Training loss: 3472.22, train acc: 0.9538, time: 143.03s\n",
            "Epoch:9, Training loss: 3212.84, train acc: 0.9535, time: 141.31s\n",
            "Epoch:10, Training loss: 2894.63, train acc: 0.9592, time: 142.33s\n",
            "Epoch:11, Training loss: 2822.80, train acc: 0.9609, time: 145.85s\n",
            "Epoch:12, Training loss: 2533.53, train acc: 0.9619, time: 145.36s\n",
            "Epoch:13, Training loss: 2368.93, train acc: 0.9610, time: 140.95s\n",
            "Epoch:14, Training loss: 2105.42, train acc: 0.9633, time: 142.04s\n",
            "Epoch:15, Training loss: 2049.23, train acc: 0.9600, time: 145.48s\n",
            "Epoch:16, Training loss: 1838.20, train acc: 0.9647, time: 143.44s\n",
            "Epoch:17, Training loss: 1782.33, train acc: 0.9648, time: 142.37s\n",
            "Epoch:18, Training loss: 1631.44, train acc: 0.9668, time: 142.68s\n",
            "Epoch:19, Training loss: 1506.01, train acc: 0.9686, time: 143.38s\n",
            "Epoch:20, Training loss: 1406.69, train acc: 0.9802, time: 147.44s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IY2YEXHltER",
        "colab_type": "text"
      },
      "source": [
        "# Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NpwVb3YlsZ5",
        "colab_type": "code",
        "outputId": "437225ab-0683-4353-b78b-530ede1d75a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpAwki6blv2y",
        "colab_type": "code",
        "outputId": "ffc87088-1319-4f9a-c675-414e887a4829",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Save the Model\n",
        "torch.save(model5, '/content/gdrive/My Drive/NLP_Assignment2/NER_Model5.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type BiLSTM_CRF. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOGcSWf8l8dR",
        "colab_type": "text"
      },
      "source": [
        "# Load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfzwz6Tnl7qj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1Uz0CdwpzJ2FbIQD8pt-09aLMWylrlXT_'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('NER_Model5.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE6eONr-mCp2",
        "colab_type": "code",
        "outputId": "fddc8fd9-94b5-487e-8084-2e37228487ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "# Load the model\n",
        "NERModel5 = torch.load('NER_Model5.pt')\n",
        "\n",
        "# Evaluate the model\n",
        "NERModel5.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTM_CRF(\n",
              "  (word_embeds): Embedding(13972, 51)\n",
              "  (lstm): LSTM(51, 25, num_layers=2, dropout=0.2, bidirectional=True)\n",
              "  (hidden2tag): Linear(in_features=50, out_features=7, bias=True)\n",
              "  (embedding): Embedding(13972, 50)\n",
              "  (out): Linear(in_features=100, out_features=50, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YOZfleDuG6z",
        "colab_type": "text"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxyNPw7Xkxnq",
        "colab_type": "text"
      },
      "source": [
        "## Testing on Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvIG3rKp6ufz",
        "colab_type": "code",
        "outputId": "d6972aea-1924-47e1-8416-18c174428a34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "y_true, y_pred, _ = cal_acc(NERModel5, val_input_index,val_output_index)\n",
        "\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       I-LOC     0.7490    0.9260    0.8282       419\n",
            "      I-MISC     0.8322    0.6631    0.7381       187\n",
            "       I-ORG     0.7724    0.3930    0.5209       285\n",
            "       I-PER     0.9770    0.8251    0.8947       875\n",
            "           O     0.9577    0.9933    0.9752      5790\n",
            "\n",
            "    accuracy                         0.9393      7556\n",
            "   macro avg     0.8577    0.7601    0.7914      7556\n",
            "weighted avg     0.9383    0.9393    0.9347      7556\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl7xmBIPtczt",
        "colab_type": "text"
      },
      "source": [
        "## Prediction on Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-Q4R2uatiF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "test_input_index = to_index(test_data_tokenized, word_to_ix)\n",
        "\n",
        "import numpy as np\n",
        "def calAccuracy_test(model, input_index):\n",
        "    predicted = []\n",
        "    for i, idxs in enumerate(input_index):\n",
        "        _, pred = model(torch.tensor(idxs, dtype=torch.long).to(device))\n",
        "        predicted += pred\n",
        "    return predicted\n",
        "\n",
        "y_pred_test = calAccuracy_test(NERModel5, test_input_index)\n",
        "\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "\n",
        "test_output = decode_output(y_pred_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3gPse2wttAZ",
        "colab_type": "text"
      },
      "source": [
        "## Write predictions to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWQRy4JhtwS3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#**********************************RUN THIS CELL**********************************#\n",
        "\n",
        "predicted_file = pd.DataFrame(columns = ['Id','Predicted'])\n",
        "predicted_file['Predicted'] = test_output\n",
        "predicted_file['Id'] = np.arange(0, len(test_output))\n",
        "predicted_file.to_csv('Model5.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}